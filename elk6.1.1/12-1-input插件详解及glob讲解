input 插件指定数据输入源 一个pipeline 可以有多个input插件 我们主要讲解下面
几个input插件

-stdin
-file
-kafka


最简单的输入 从标准输入读取数据 通用配置为：
-codec 类型为 codec
-type 类型为string 自定义该事件的类型 可用于后续判断 mysql  type mysql
-tags 类型为array 自定义该事件的tag 可用于后续判断  filter 使用
-add_field 类型为 hash 为该事件添加字段


示例：

cat > /opt/logstash-6.1.1/input-stdin.conf <<EOF
input{
 stdin{
 codec => "plain"  # plain 什么都不做的解码
 tags => ["test"]
 type => "std"
 add_field => {"key" => "value"}
 }
}

output {
  stdout {
  codec => "rubydebug"
  }
}
EOF


echo "test"  | /opt/logstash-6.1.1/bin/logstash -f /opt/logstash-6.1.1/input-stdin.conf




{
       "message" => "test",
           "key" => "value",
    "@timestamp" => 2019-03-14T07:00:09.008Z,
          "host" => "localhost.localdomain",
      "@version" => "1",
          "tags" => [
        [0] "test"
    ],
          "type" => "std"
}


input plugin  -  file
从文件读取数据 如常见的日志文件 文件读取通常要解决几个问题
-文件内如如何只被读取一次 即重启logstash时 从上次读取的位置继续

-sincedb

-如何即时读取到文件的新内容
-定时检车文件是否有更新

-如何返现新文件并进行读取
-可以 定时检查新文件


-如果文件发生了归档操作 rotation 是否影响当前的内容读取
-不影响 被归档的文件内容可以继续被读取
基于innode  和文件名没有关系


path 类型为数组 指明读取文件的路径 基于glob 匹配语法
-path => ["/var/log/**/*.log","/var/log/message"]

exclue 类型为数组排除不想监听的文件规则 基于glob 匹配语法
- exclude => "*.gz"

sincedb_path 类型为字符串 记录sincedb 文件路径
start_postion 类型为字符串 begging or end 是否从头读取文件
stat_interval 类型为数值 单位 秒 定时检查文件是否有更新 默认1秒

discover_interval 类型为数值 单位秒 定时检查是否有新文件待读取 默认15秒
ignore_older 类型为数值单位秒 扫描文件列表时 如果该文件上次更改时间超过设定时长 则不做处理
单依然会监控会否有新内容 默认关闭

close_older 类型为数值 单位秒 如果监听的文件在超过该设定的时间内没有更新内容
会关闭文件句柄 释放资源 但已然会监控是否有新内容 默认3600秒即1个小时



glob 匹配语法

- * 匹配任意字符 但不匹配以 . 开头的隐藏文件 匹配这类文件时 以 .* 来匹配

- ** 递归匹配子目录
- ? 匹配单一字符
- [] 匹配多个字符 比如 [a-z]  [^a-Z]
- {} 匹配多个单词 比如 {foo,bar,hello}
- \ 转义符号


示例
"/var/log/*.log"
匹配/var/log 目录下以.log 结尾的文件

"/var/log/**/*.log"
匹配/var/log 所有子目录下 以 .log 结尾的文件

"/var/log/{app1,app2,app3}/*.log"
匹配 /var/log/目录西 app1 ap2 app3 的目录中以 .log 结尾的文件



input{
 file{
 path => ["/var/log/access_log","/var/log/err_log"],
 type => "web",
 start_position => "beginning"
 }
}


start_position 只有在第一次读取的时候 begging才生效
一旦读取过这个文件 在sincedb 里面有记录了
再次运行的时候 即便 是begging 也不会去从头读取  也会从sincedb 去读取

删除 sincedb

或者如下配置


input{
 file {
     path => "/var/logs/*.log"
     sincedb => "/dev/null"
     start_position => "beginning"
     ignore_older =>0
     close_older => 5
     discover_interval => 1
  }
}

output {
  stdout {codec => rubydebug{}}
}


input kafka example

input {
  kafka {
   zk_connect => "kafka:2181"
   group_id => "logstash"
   topic_id => "apache_logs"
   consumer_threads => 16
 }
}
































































