filebeat -> logstash -> es -> kibana

filebeat -> ingest node 数据处理

es ingest node
-在数据写入es前(bulk/index 操作) 对数据进行处理
-可设置独立的ingest node 专门负责数据转换处理 node.ingest:true
-api endpoint 为pipeline


pipeline 由一系列processor组成 类似logstash 的filter plugin

{
"description":"...",
"processors":[...]
}

pipeline 的api 比较简单 主要如下4个：
创建 PUT
获取 GET
删除 DELETE
模拟调试 SIMULATE


PUT _ingest/pipeline/test
{
 "description":"for test",
 "processors":[
 {
   "set":{
     "field":"name",
     "value":"alfred"
     }
  }

 ]
}


set  设置一个字段 名字 name  值 alfred

GET _ingest/pipeline/test
DELETE _ingest/pipeline/test


simulate 模拟调试

模拟调试时 写入 pipeline

POST _ingest/pipeline/_simulate
{
 "pipeline":{
   "description":"for test",
   "processors":[
    {
     "set":{
       "field":"name",
       "value":"alfred"
      }
    }
  ]
},

"doc":[
{
"_source":{
 "message":"my message"
 }
 }
 ]
}



指定pipeline 模拟调试

POST _ingest/pipeline/test/_simulate
{
"docs":[
{
 "_source":{
 "message":"my message"
 }
 }
 ]
 }



实例 创建pipeline

sudo -u elasticsearch /opt/elasticsearch-6.1.1/bin/elasticsearch -E cluster.name=my_cluster_test -E node.name=test -Ehttp.host=172.16.99.2 -Ehttp.port=9200 -Epath.data=/tmp/my_cluster_node_test  -v

/opt/kibana-6.1.1-linux-x86_64/bin/kibana -e http://172.16.99.2:9200 -H 172.16.99.2 -p 5601


PUT _ingest/pipeline/test
{
  "description": "for test",
  "processors": [
    {
      "set": {
        "field": "name",
        "value": "alfred"
      }
    }
  ]
}


GET _ingest/pipeline/test

DELETE _ingest/pipeline/test




POST _ingest/pipeline/_simulate
{
  "pipeline": {
    "description": "for test",
    "processors": [
      {
        "set": {
          "field": "name",
          "value": "alfred"
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {
        "message": "my message"
      }
    }
  ]
}




POST _ingest/pipeline/test/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "my message"
      }
    }
  ]
}






ingest node pipeline

Processor 对应 logstash 的 filter plugin 基本都涵盖了
-Convert
-Grok
-date
-gsub
-join
-json
-remove
-script  ruby code


注意吃力解析错误的情况
on_failure  和 processors 平级

processors 下一级


日志处理中常用到的两个es插件

bin/elasticsearch-plugin install ingest-geoip
bin/elasticsearch-plugin install ingest-user-agent

使用 es  ingest node 将nginx 日志转换成json

_source 三个双引号 里面内容都是字符串



pipeline 的使用比较简单 在索引相关的api中都有 pipeline 的参数可以指定

PUT my-index/doc/1?pipeline=test
{
"message":"test"
}

POST _bulk?pipeline=test
{
"message":"test"
}


POST _reindex
{
"source":{
"index":"source"
},
"dest":{
"index":"dest",
"pipeline":"some_ingest_pipeline"
}
}

filebeat可以在output.elasticsearch 中指定piepeline

output.elasticsearch:
hosts:["localhost:9200"]
pipeline:my_pipeline_id
























