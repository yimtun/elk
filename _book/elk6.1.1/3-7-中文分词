难点
-中文分词指的是将一个汉字序列切分为一个一个单独的词
在英文中 单词之间以空格作为自然分界符 汉语中没有一个形式上的分界符

-上下文不同 分词结果迥异 比如交叉歧义的问题 比如下面两种分词都合理
-乒乓球拍/卖/完了
-乒乓球/拍卖/完了

es中使用中文分词 可以使用的分词器

-IK
实现中英文单词的切分 支持ik_smart ik_maxword 等模式
可自定义词库 支持热更新分词词典
https://github.com/medcl/elasticsearch-analysis-ik

-jeiba
python中流行的分词系统 支持分词和词性标注
支持繁体分词 自定义词典 并行分词等
https://github.com/singlee/elasticsearch-jieba-plugin


基于自然语言处理的分词系统
-hanlp
由一系列模型与算法组成的java工具包 目标是普及自然语言处理在生产环境中
的应用
https://github.com/hankcs/HanLP

-THULAC
-THU Lexical Analyzer for Chinese 由清华大学自然语言处理与社会人文计算实验室
研制推出的一套中文词法分析工具包 具有中文分词和词性标注功能
https://github.com/microbun/elasticsearch-thulac-plugin



